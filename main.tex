\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{CTEX}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
% \usepackage[backref]{hyperref}
\usepackage[colorlinks,
            linkcolor=green,
            anchorcolor=green,
            citecolor=green,
            backref=page]{hyperref}
\usepackage{fancyhdr}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}



\begin{document}
% \bibliographystyle{plain}

\title{图神经网络在动态图上的链路预测综述}

\author{
	\IEEEauthorblockN{王天赐}
	\IEEEauthorblockA{
		\textit{计算机学院} \\
		\textit{湖北工业大学} \\
		武汉,湖北,中国 \\
	}
}

\maketitle

\thispagestyle{fancy} % IEEE模板在\maketitle后会自动声明\thispagestyle{plain}，
% 导致第一页什么都没有。所以得把plain更改为fancy
\lhead{} % 页眉左，需要东西的话就在{}内添加
\chead{} % 页眉中
\rhead{} % 页眉右
\lfoot{} % 页脚左
\cfoot{\thepage} % 页脚中
\rfoot{} %页脚右，\thepage 表示当前页码
\renewcommand{\headrulewidth}{0pt} %改为0pt即可去掉页眉下面的横线
\renewcommand{\footrulewidth}{0pt} %改为0pt即可去掉页脚上面的横线

\pagestyle{fancy}
\cfoot{\thepage}


\begin{abstract}
图形神经网络（GNNs）正迅速成为在图形结构数据上学习的主导方式。链接预测是新的GNN模型的一个几乎通用的基准。
许多先进的模型，如动态图神经网络（DGNNs）专门针对动态链接预测。	然而，这些模型，特别是DGNNs，很少与其他模型或现有启发式方法进行比较。不同的工作以不同的方式评估他们的模型，因此人们无法直接比较评估指标。受此启发，我们进行了一次全面的比较研究。我们比较了链接预测启发式方法、GNNs、离散DGNNs和连续DGNNs对动态链接预测的作用。我们发现，简单的链接预测启发式方法往往比GNN和DGNN表现更好，而在所有被考察的图神经网络中，DGNN的表现一直优于静态GNN。
\end{abstract}

\begin{IEEEkeywords}
动态图神经网络; 启发式; 链接预测;
\end{IEEEkeywords}

\section{引言}
\subsection{研究背景}
在本文中主要的研究内容是在动态图上的链接预测, 与静态图的链接预测不同的是, 动态图的链接预测增加了时间维度, 预测难度增加, 但是在现实中的应用更加广泛。

\subsection{研究方法}
在近年来的研究中, 很多链路预测方法取得了发展, 在静态图上研究者们提出了很多高精度的链接预测方法。
比如基于启发式算法的共同邻居算法\cite{papadopoulos2015network}, 
以及首次将GNN应用在图的链接预测的SEAL模型\cite{zhang2018link}, SEAL模型通过将GNN与启发式算法融合在静态图的预测上有很
高的精度。动态图的链接预测增加了时间维度, 不同的时间段的图的结构和链接节点之间有差异\cite{peng2021dynamic}, 所以这导致了在动态图的预测的难度大大增加。
为了解决这些问题, 研究者们提出了很多动态预测的方法。有研究者提出GC-LSTM\cite{chen2018gc}, 通过融合GCN和LSTM来处理动态图的时间序列, 
也有研究者通过将GCN分别嵌入LSTM和GRU\cite{pareja2020evolvegcn}在不同的任务场景上得到很好的效果。


\subsection{研究问题}

近年来，图神经网络（GNNs）作为一个新兴的研究领域，得到了长足的发展，很多研究者提出并发展了多种体系结构\cite{zhang2019graph}。
但是在动态图神经网络（DGNN）领域，这些问题因以下原因而进一步加剧：


\subsubsection{缺乏既定的强大基线（大多数研究不与其他DGNN比较性能）}
动态链接预测任务基线的不统一, 不同的工作中使用的BaseLine不同, 导致对现有工作进行合理的评估。
\subsubsection{大量的实验设计选择}
这些选择包括：如何表示动态网络（如快照、时间窗口、连续、边的生存时间等），包括哪些节点特征，如何将数据分成训练-验证-测试集，用哪些指标来评价结果，如何在报告的指标中使用负采样率，以及如何选择/优化神经网络参数（如学习率、早期停止准则、嵌入空间维度等）。所有这些都意味着，通过阅读研究论文来比较方法的性能是不可能的，除非他们清楚地说明他们所有的设计选择，而且这些设计选择在不同的论文中是相同的。
DGNNs是建立网络动态模型的一个很有前途的途径，因为它们既能通过GNNs编码空间模式，又能通过时间序列组件（如循环神经网络（RNN）或自我注意）编码时间模式。然而，迄今为止提出的DGNNs已经在少数数据集上进行了测试，并且很少与其他DGNNs进行比较。不同的研究在不同的数据集上进行比较，因为在DGNN基准测试中使用哪种数据集的问题上没有共识。

\section{相关研究模型与数据集}

\subsection{相关数据集}
我们选择了五个连续的交互网络和一个离散的演化网络作为数据集。我们选择了互动网络，因为它们允许我们轻松地转换为更粗粒度的时间粒度，如离散网络。更稀疏的快照表明链接和非链接之间有更大的不平衡性，从而使分类问题更难。我们为每个数据集准备了两个版本，一个是有方向的连续交互网络，一个是无方向的离散网络。连续模型对连续网络进行编码。静态和离散模型对离散网络进行编码。在从连续到离散的转换中，互换的边缘被添加到离散网络中，使其成为无定向的。一条边在快照中出现的次数被作为权重加到快照的边上。
所有的结果都是对离散网络的预测报告。对于连续模型，这是通过将连续网络的连续部分分割成与离散网络中的快照相对应的快照来实现的。然后，我们让连续模型在目标快照前对连续网络进行编码，然后尝试预测离散网络中的链接发生。
在本文中，我们使用了以下数据集：

\subsubsection{Enron}
这个数据集是一个电子邮件通信网络，其中一个链接是两个人之间发送的电子邮件。Enron在空间上是一个很小的网络，但在时间上是一个中等规模的网络，有合理数量的连续链接，覆盖的时间跨度超过3年。
由于节点数量少，边的数量相对较多，它比其他网络要密集得多。

\subsubsection{UC Irvine Messages}
简称UC，是加州大学欧文分校的一个在线论坛网络。如果两个学生在同一个论坛帖子上互动，他们就会被连接起来。
因此，这原本是一个二方网络，但它被预测为只有一种类型的节点。快照大小的奇特选择来自于EvolveGCN（Pareja等人，2020），它观察到较小的快照大小会产生一些没有任何边的快照

\subsubsection{Bitcoin-OTC}
这个数据集是一个在比特币OTC平台上交易的谁信任谁的网络。一个链接是一个用户对另一个用户的评价。
比特币网络在节点方面是中等规模的，然而，它的大多数边是唯一的边，这表明很少有边是重复出现的。缺少重复出现的边缘导致每个快照比其他大多数数据集要稀疏得多。

\subsubsection{Autonomous-systems}
这个数据集是一个互联网路由器通信网络。一个链接是一个路由器与一个对等体交换流量。这个网络已经被汇总为一个离散的网络并选择前99天，并将其作为我们的数据集。这是迄今为止拥有最多边缘的数据集。

\subsubsection{Wikipedia}
这个数据集是一个双联的维基百科页面编辑网络。节点是一个维基百科用户或一个维基百科页面。一个链接是一个编辑维基百科页面的用户。维基百科也有很少的重现边缘，与比特币类似，然后有比较稀疏的时间快照。

\subsubsection{Reddit}
这个数据集是一个双联的Reddit发布网络。节点是一个Reddit用户或一个subreddit。一个链接是一个用户在一个子reddit上的发帖。Reddit是空间上最大的网络，因为它有最大数量的节点和独特的边。


\subsection{相关研究模型}
\subsubsection{GCN}
GCN\cite{kipf2016semi}借用了卷积神经网络（CNN）的 卷积概念，直接根据图的连通结构对图进行卷积，作为滤波器进行邻域混合

\subsubsection{GAT}
GAT\cite{velivckovic2017graph}将注意机制与GNN相结合，旨在 更有效地学习邻里特征。图形注意层作为GAT的组成部 分，对GNN起着聚合作用。它首先对每个节点应用一 个共享的线性变换，由权重矩阵W参数化∈射频0×F

\subsubsection{GC-LSTM}
GC-LSTM\cite{chen2018gc}中的GCN能够为每个时间段滑动节点结构学习网络快照，而LSTM负责网络快照的时间特征学习。
此外，当前的动态链路预测方法只能处理删除的链接，GC-LSTM可以同时预测添加或删除的链接。

\subsubsection{TGAT}
时间图注意力TGAT\cite{cui2021dygcn}通过有效地聚合时间拓扑邻域特征以及学习时间特征交互通过堆叠 TGAT 层，网络将节点嵌入识别为时间的函数，并能够随着图的发展归纳推断新节点和观察到的节点的嵌入。
TGAT可以同时处理节点分类和链接预测任务，并且可以自然地扩展到包括时间边缘特征。


\section{相关方法}

\subsection{Graph Convolutional Networks (GCN)}
GCN\cite{kipf2016semi}借用了卷积神经网络（CNN）的卷积概念，直接根据图的连通结构对图进行卷积，作为滤波器进行邻域混合。架构可以简单地概括为
\begin{equation}
H^{l+1}=\sigma\left(\tilde{D}^{-1 / 2} \tilde{A}^{-1 / 2} \tilde{D}^{-1 / 2} H^{(l)} W^{(l)}\right)
\end{equation}
在这里σ 表示sigmoid函数，u和v是两个邻居，而vn是负样本，Q是负样本数。第一项的目标是最大化u和v的嵌入之间的相似性，而第二项则试图区分负样本的嵌入

\subsubsection{Graph Attention Networks (GAT)}
GAT\cite{velivckovic2017graph}将注意机制与GNN相结合，旨在更有效地学习邻里特征。图形注意层作为GAT的组成部分，对GNN起着聚合作用。它首先对每个节点应用一个共享的线性变换，由权重矩阵W参数化∈ 射频0×F。然后在节点上执行自我注意，其中使用共享注意机制来计算捕获节点j的特征对节点i的重要性的注意系数，即：
\begin{equation}
e_{i j}=a\left(\mathbf{W} h_{i}, \mathbf{W} h_{j}\right)
\end{equation}
GAT使用softmax函数对j上的系数进行归一化。因此，注意机制是一个由权重向量a参数化的单层网络∈ R2F 0，然后进行非线性激活（例如LeakyReLU），并获得归一化系数，如下所示：
\begin{equation}
\alpha_{i j}=\frac{\exp \left(\operatorname{LeakyReLU}\left(a\left[\mathbf{W} h_{i}, \mathbf{W} h_{j}\right]\right)\right)}{\sum_{k \in N_{i}} \exp \left(\operatorname{LeakyReLU}\left(a\left[\mathbf{W} h_{i}, \mathbf{W} h_{k}\right]\right)\right)}
\end{equation}
这些注意系数用于计算邻居特征的线性组合以获得每个节点的聚集特征，即：
\begin{equation}
h_{i}^{\prime}=\sum_{k \in N_{i}} \alpha_{i j} \mathbf{W} h_{k}
\end{equation}
最后，多头注意（即一套独立的注意机制）被用来稳定自我注意的学习过程。而多头注意力需要用到的算力可能有些相对较大了，不一定说一定适合个人训练，可以使用相关领域的预处理模型来迁移学习。

\subsubsection{Graph Convolution Embedded LSTM}
LSTM被作为主要框架来学习动态网络的所有快照的时间特征。而对于每个快照，GCN被用来捕捉节点的局部结构特性以及它们之间的关系。一个好处是，GC-LSTM可以预测添加和删除的链接。GCN最初是为无定向网络设计的，这意味着对称网络的拉普拉斯矩阵。
为了将GCN嵌入提议的GC-LSTM中，我们首先需要重新定义拉普拉斯矩阵。按照Ma等人的想法，有向拉普拉斯矩阵被定义为 :
\begin{equation}
	L^{s y m}=I-\frac{1}{2}\left(\Phi^{1 / 2} P \Phi^{-1 / 2}+\Phi^{-1 / 2} P^{T}\right.
\end{equation}
GC-LSTM模型主要依靠两个状态值，一个是用于提取最后一次输入信息的隐藏状态h，一个是用于保存长期信息的单元状态c。GC-LSTM的本质是它在前进过程中有一个单元状态c，导致信息在单元状态c上长期传输。
单元状态可以通过遗忘门和输入门进行更新，定义如下:
\begin{equation}
\begin{aligned}
	&\bar{c}_{t}=\tanh \left(A_{t} W_{c}+G C N_{o}^{K}\left(\tilde{A}_{t-1}, h_{t-1}\right)+b_{c}\right), \\
	&i_{t}=\sigma\left(A_{t} W_{i}+G C N_{c}^{K}\left(\tilde{A}_{t-1}, h_{t-1}\right)+b_{i}\right) \\
	&c_{t}=f_{t} \odot G C N_{c}^{K} c_{t-1}+i_{t} \cdot \bar{c}_{t} .
\end{aligned}
\end{equation}
在动态网络链接预测任务中，我们需要考虑邻居的隐藏状态对节点隐藏状态的影响，以及邻居的细胞状态的影响。


\subsubsection{TGAT}
TGAT可以有效地聚集时间-拓扑邻域特征，并学习时间特征的相互作用。通过使用自我注意机制作为构建模块，并在经典的Bochner定理的基础上开发了一种新的功能性时间编码技术，即谐波分析法。通过堆叠TGAT层，网络将节点嵌入识别为时间函数，并且能够在图的演变过程中归纳推断新节点和观察到的节点的嵌入。
根据原始的自我注意机制，我们首先得到实体-时间特征矩阵为:
\begin{equation}
	\mathbf{Z}(t)=\left[\tilde{\mathbf{h}}_{0}^{(l-1)}(t)\left\|\Phi_{d_{T}}(0), \tilde{\mathbf{h}}_{1}^{(l-1)}\left(t_{1}\right)\right\| \Phi_{d_{T}}\left(t-t_{1}\right), \ldots, \tilde{\mathbf{h}}_{N}^{(l-1)}\left(t_{N}\right) \| \Phi_{d_{T}}\left(t-t_{N}\right)\right]^{\top}
\end{equation}
并将其转发给三个不同的线性投影，以获得 'query','key' 和 'value'。
\begin{equation}
	\mathbf{q}(t)=[\mathbf{Z}(t)]_{0} \mathbf{W}_{Q}, \mathbf{K}(t)=[\mathbf{Z}(t)]_{1: N} \mathbf{W}_{K}, \mathbf{V}(t)=[\mathbf{Z}(t)]_{1: N} \mathbf{W}_{V}
\end{equation}
然后，我们将其传递给一个前馈神经网络，以捕捉特征之间的非线性相互作用，如:
\begin{equation}
	\begin{aligned}
	&\tilde{\mathbf{h}}_{0}^{(l)}(t)=\operatorname{FFN}\left(\mathbf{h}(t) \| \mathbf{x}_{0}\right) \equiv \operatorname{ReLU}\left(\left[\mathbf{h}(t) \| \mathbf{x}_{0}\right] \mathbf{W}_{0}^{(l)}+\mathbf{b}_{0}^{(l)}\right) \mathbf{W}_{1}^{(l)}+\mathbf{b}_{1}^{(l)}, \\
	&\mathbf{W}_{0}^{(l)} \in \mathbb{R}^{\left(d_{h}+d_{0}\right) \times d_{f}}, \mathbf{W}_{1}^{(l)} \in \mathbb{R}^{d_{f} \times d}, \mathbf{b}_{0}^{(l)} \in \mathbb{R}^{d_{f}}, \mathbf{b}_{1}^{(l)} \in \mathbb{R}^{d}
\end{aligned}
\end{equation}
因此，TGAT层可以使用半监督学习框架实现节点分类任务，也可以使用编码器-解码器框架实现链接预测任务。


\section{结论}
在本文中我们介绍并实现几种现有的离散和连续DGNNs的框架。我们对不同的GNN在动态链接预测任务上进行了综合比较。我们的实验表明，这些GNN架构在 其他链路预测任务的基准测试中表现相似。本文的未来有几个有趣的方向。首先，我们的基准数据集仍然相 对较小，将来我们可以在更大的图上评估模型，特别 是在现实世界中的应用。
本文的第二个有趣的方向是 实现最近开发的GNN模型。此外，我们可以尝试设计和开发我们自己的DGNN架构和链路预测任务的基准测试。	

\begin{thebibliography}{00}
\bibitem{jeon2020efficient}
G.~Jeon, H.~Jeong, and J.~Choi, ``An efficient explorative sampling considering
the generative boundaries of deep generative neural networks,'' in
\emph{Proceedings of the AAAI Conference on Artificial Intelligence},
vol.~34, no.~04, 2020, pp. 4288--4295.

\bibitem{kipf2016semi}
T.~N. Kipf and M.~Welling, ``Semi-supervised classification with graph
convolutional networks,'' \emph{arXiv preprint arXiv:1609.02907}, 2016.

\bibitem{velivckovic2017graph}
P.~Veli{\v{c}}kovi{\'c}, G.~Cucurull, A.~Casanova, A.~Romero, P.~Lio, and
Y.~Bengio, ``Graph attention networks,'' \emph{arXiv preprint
	arXiv:1710.10903}, 2017.

\bibitem{zhang2018link}
M.~Zhang and Y.~Chen, ``Link prediction based on graph neural networks,''
\emph{Advances in neural information processing systems}, vol.~31, 2018.

\bibitem{xu2020inductive}
D.~Xu, C.~Ruan, E.~Korpeoglu, S.~Kumar, and K.~Achan, ``Inductive
representation learning on temporal graphs,'' \emph{arXiv preprint
	arXiv:2002.07962}, 2020.

\bibitem{chen2018gc}
J.~Chen, X.~Wang, and X.~Xu, ``Gc-lstm: Graph convolution embedded lstm for
dynamic link prediction,'' \emph{arXiv preprint arXiv:1812.04206}, 2018.

\bibitem{pareja2020evolvegcn}
A.~Pareja, G.~Domeniconi, J.~Chen, T.~Ma, T.~Suzumura, H.~Kanezashi, T.~Kaler,
T.~Schardl, and C.~Leiserson, ``Evolvegcn: Evolving graph convolutional
networks for dynamic graphs,'' in \emph{Proceedings of the AAAI Conference on
	Artificial Intelligence}, vol.~34, no.~04, 2020, pp. 5363--5370.

\bibitem{zhang2019graph}
S.~Zhang, H.~Tong, J.~Xu, and R.~Maciejewski, ``Graph convolutional networks: a
comprehensive review,'' \emph{Computational Social Networks}, vol.~6, no.~1,
pp. 1--23, 2019.

\bibitem{gao2022novel}
C.~Gao, J.~Zhu, F.~Zhang, Z.~Wang, and X.~Li, ``A novel representation learning
for dynamic graphs based on graph convolutional networks,'' \emph{IEEE
	Transactions on Cybernetics}, 2022.

\bibitem{cui2021dygcn}
Z.~Cui, Z.~Li, S.~Wu, X.~Zhang, Q.~Liu, L.~Wang, and M.~Ai, ``Dygcn: dynamic
graph embedding with graph convolutional network,'' \emph{arXiv preprint
	arXiv:2104.02962}, 2021.

\bibitem{peng2021dynamic}
H.~Peng, B.~Du, M.~Liu, M.~Liu, S.~Ji, S.~Wang, X.~Zhang, and L.~He, ``Dynamic
graph convolutional network for long-term traffic flow prediction with
reinforcement learning,'' \emph{Information Sciences}, vol. 578, pp.
401--416, 2021.

\bibitem{qiu2020dgcn}
Z.~Qiu, K.~Qiu, J.~Fu, and D.~Fu, ``Dgcn: Dynamic graph convolutional network
for efficient multi-person pose estimation,'' in \emph{Proceedings of the
	AAAI Conference on Artificial Intelligence}, vol.~34, no.~07, 2020, pp.
11\,924--11\,931.

\bibitem{papadopoulos2015network}
F.~Papadopoulos, R.~Aldecoa, and D.~Krioukov, ``Network geometry inference
  using common neighbors,'' \emph{Physical Review E}, vol.~92, no.~2, p.
  022807, 2015.

\end{thebibliography}


% \bibliography{ref}
\end{document}
